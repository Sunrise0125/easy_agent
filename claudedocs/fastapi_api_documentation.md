# PaperFinder Agent - API è°ƒç”¨æ–‡æ¡£

**æœåŠ¡åœ°å€**: `http://localhost:8000` (é»˜è®¤)
**æ–‡æ¡£æ›´æ–°**: 2025-11-15

---

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡

```bash
# é…ç½®ç¯å¢ƒå˜é‡ (.env æ–‡ä»¶)
OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=https://api.deepseek.com/v1
OPENAI_MODEL=deepseek-chat
S2_API_KEY=your_s2_api_key
MAX_RESULTS_LIMIT=500  # æœ€å¤§è®ºæ–‡æ•°é‡é™åˆ¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤ 500ï¼‰

# å¯åŠ¨æœåŠ¡
uvicorn main:app --reload --port 8000
```

### 2. è®¿é—®æ–‡æ¡£

- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

---

## API è°ƒç”¨

### åŒæ­¥æ¥å£ï¼ˆç«‹å³è¿”å›ç»“æœï¼‰

#### æ¥å£åœ°å€

```
GET /search?user_query={æŸ¥è¯¢å†…å®¹}
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| user_query | string | âœ… | è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œæ”¯æŒä¸­è‹±æ–‡ |

**é€‚ç”¨åœºæ™¯**: ç®€å•æŸ¥è¯¢ã€å¿«é€Ÿè·å–ç»“æœ

---

### å¼‚æ­¥æ¥å£ï¼ˆåå°æ‰§è¡Œï¼Œæ”¯æŒè¿›åº¦è·Ÿè¸ªï¼‰

#### 1. åˆ›å»ºæœç´¢ä»»åŠ¡

```
POST /tasks/search
Content-Type: application/json
```

**è¯·æ±‚ä½“**:
```json
{
  "user_query": "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼Œæœ€è¿‘ä¸‰å¹´"
}
```

**å“åº”** (HTTP 202 Accepted):
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "created",
  "created_at": "2025-11-16T10:00:00Z"
}
```

#### 2. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```
GET /tasks/{task_id}
```

**å“åº”**ï¼ˆè¿›è¡Œä¸­ï¼‰:
```json
{
  "task_id": "550e8400-...",
  "status": "searching",
  "progress": {
    "stage": "searching",
    "stage_description": "æ­£åœ¨æœç´¢ OpenAlex",
    "sources": {
      "s2": {
        "status": "completed",
        "fetched": 120,
        "total_estimated": null,
        "errors": null
      },
      "openalex": {
        "status": "in_progress",
        "fetched": 45,
        "total_estimated": 100,
        "errors": null
      },
      "arxiv": {
        "status": "pending",
        "errors": null
      }
    },
    "overall_percent": 58
  },
  "created_at": "2025-11-16T10:00:00Z",
  "updated_at": "2025-11-16T10:00:05Z"
}
```

**å“åº”**ï¼ˆå®Œæˆï¼‰:
```json
{
  "task_id": "550e8400-...",
  "status": "completed",
  "progress": {
    "stage": "completed",
    "stage_description": "æœç´¢å®Œæˆ",
    "overall_percent": 100
  },
  "results": {
    "query": "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼Œæœ€è¿‘ä¸‰å¹´",
    "normalized_intent": { ... },
    "api_params": { ... },
    "counts": { ... },
    "results": [ ... ]
  },
  "errors": [],
  "created_at": "2025-11-16T10:00:00Z",
  "completed_at": "2025-11-16T10:00:12Z"
}
```

#### ä»»åŠ¡çŠ¶æ€è¯´æ˜

| çŠ¶æ€ | è¯´æ˜ | è¿›åº¦ | ä¸­æ–‡æè¿°ç¤ºä¾‹ |
|------|------|------|-------------|
| `created` | ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ | 0% | "ä»»åŠ¡å·²åˆ›å»º" |
| `parsing` | LLM æ­£åœ¨è§£ææŸ¥è¯¢æ„å›¾ | 25% | "æ­£åœ¨è§£ææŸ¥è¯¢æ„å›¾" |
| `searching` | å¤šæ¥æºæœç´¢è¿›è¡Œä¸­ | 25%-75% | "æ­£åœ¨æœç´¢ Semantic Scholar" |
| `ranking` | ç»“æœæ’åºä¸­ | 75%-100% | "è¿›å…¥æ’åºé˜¶æ®µ" |
| `completed` | æœç´¢å®Œæˆï¼Œç»“æœå¯ç”¨ | 100% | "æœç´¢å®Œæˆ" |
| `failed` | ä»»åŠ¡å¤±è´¥ï¼ˆæŸ¥çœ‹ error å­—æ®µï¼‰ | 0% | "æœç´¢å¤±è´¥" |

#### æ¥æºçŠ¶æ€è¯´æ˜

| çŠ¶æ€ | è¯´æ˜ |
|------|------|
| `pending` | ç­‰å¾…å¼€å§‹ |
| `in_progress` | æ£€ç´¢ä¸­ |
| `completed` | è¯¥æ¥æºå®Œæˆ |
| `failed` | è¯¥æ¥æºå¤±è´¥ï¼ˆä¸å½±å“æ•´ä½“ä»»åŠ¡ï¼‰ |

#### ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸ

- ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥åï¼Œå°†åœ¨ **30 åˆ†é’Ÿåè‡ªåŠ¨æ¸…ç†**
- å»ºè®®å®¢æˆ·ç«¯è·å–ç»“æœåç«‹å³ä¿å­˜
- ä»»åŠ¡ ID é‡‡ç”¨ UUID4 æ ¼å¼ï¼Œéšæœºç”Ÿæˆ

**é€‚ç”¨åœºæ™¯**: å¤æ‚æŸ¥è¯¢ã€éœ€è¦å®æ—¶è¿›åº¦åé¦ˆçš„å‰ç«¯åº”ç”¨

---

## è°ƒç”¨ç¤ºä¾‹

### cURL - åŒæ­¥æ¥å£

```bash
# åŸºç¡€æŸ¥è¯¢
curl "http://localhost:8000/search?user_query=æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼Œæœ€è¿‘ä¸‰å¹´"

# å¤æ‚æŸ¥è¯¢
curl "http://localhost:8000/search?user_query=å¼ºåŒ–å­¦ä¹ ä¸æœºå™¨äººæ§åˆ¶ï¼ŒICLRæˆ–NeurIPSï¼Œ2023åˆ°2025ï¼ŒæŒ‰å¼•ç”¨æ•°æ’åº"

# URL ç¼–ç æŸ¥è¯¢
curl -G "http://localhost:8000/search" \
  --data-urlencode "user_query=Transformeræ¶æ„çš„æœ€æ–°ç ”ç©¶ï¼Œè¦æ±‚æœ‰PDF"
```

### cURL - å¼‚æ­¥æ¥å£

```bash
# 1. åˆ›å»ºä»»åŠ¡
task_response=$(curl -X POST "http://localhost:8000/tasks/search" \
  -H "Content-Type: application/json" \
  -d '{"user_query": "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼Œæœ€è¿‘ä¸‰å¹´"}')

# æå– task_id
task_id=$(echo $task_response | jq -r '.task_id')
echo "ä»»åŠ¡ ID: $task_id"

# 2. è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆæ¯ 2 ç§’ä¸€æ¬¡ï¼‰
while true; do
  status=$(curl -s "http://localhost:8000/tasks/$task_id")
  task_status=$(echo $status | jq -r '.status')
  progress=$(echo $status | jq -r '.progress.overall_percent')

  echo "çŠ¶æ€: $task_status, è¿›åº¦: $progress%"

  if [ "$task_status" = "completed" ] || [ "$task_status" = "failed" ]; then
    echo $status | jq '.'
    break
  fi

  sleep 2
done
```

### Python - åŒæ­¥æ¥å£

```python
import httpx
import asyncio

async def search_papers_async(query: str):
    """çœŸæ­£çš„å¼‚æ­¥å®ç°"""
    async with httpx.AsyncClient() as client:
        response = await client.get(
            "http://localhost:8000/search",
            params={"user_query": query},
            timeout=60.0,
        )
        return response.json()

def search_papers(query: str):
    """åŒæ­¥å°è£…ï¼Œå¤–éƒ¨ç›´æ¥è°ƒç”¨è¿™ä¸ª"""
    return asyncio.run(search_papers_async(query))

if __name__ == "__main__":
    result = search_papers("æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRä¼šè®®ï¼Œæœ€è¿‘ä¸‰å¹´")

    print(f"æ‰¾åˆ° {len(result['results'])} ç¯‡è®ºæ–‡")
    for paper in result["results"]:
        print(f"æ ‡é¢˜: {paper['title']}")
        print(f"ä½œè€…: {', '.join(paper['authors'][:3])}")
        print(f"å¹´ä»½: {paper['year']}")
        print(f"å¼•ç”¨: {paper['citations']}")
        print(f"é“¾æ¥: {paper['url']}\n")

```

### Python - å¼‚æ­¥æ¥å£ï¼ˆå¸¦è¿›åº¦è·Ÿè¸ªï¼‰

```python
import httpx
import asyncio


async def search_papers_async(query: str, poll_interval: float = 2.0):
    """è°ƒç”¨å¼‚æ­¥è®ºæ–‡æ£€ç´¢ APIï¼Œæ”¯æŒè¿›åº¦è·Ÿè¸ª"""
    async with httpx.AsyncClient(timeout=120.0) as client:
        # 1. åˆ›å»ºä»»åŠ¡
        create_response = await client.post(
            "http://localhost:8066/tasks/search",
            json={"user_query": query}
        )

        if create_response.status_code != 202:
            raise Exception(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {create_response.text}")

        task_data = create_response.json()
        task_id = task_data["task_id"]
        print(f"ä»»åŠ¡å·²åˆ›å»º: {task_id}")

        # 2. è½®è¯¢ä»»åŠ¡çŠ¶æ€
        while True:
            await asyncio.sleep(poll_interval)

            status_response = await client.get(
                f"http://localhost:8066/tasks/{task_id}"
            )

            if status_response.status_code != 200:
                raise Exception(f"æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {status_response.text}")

            data = status_response.json()
            status = data.get("status", "unknown")
            progress_data = data.get("progress", {}) or {}
            progress_percent = progress_data.get("overall_percent", 0.0)
            stage_desc = progress_data.get("stage_description", "")

            print(f"çŠ¶æ€: {status} ({stage_desc}), è¿›åº¦: {progress_percent}%")

            # æ˜¾ç¤ºæ¯ä¸ªæ¥æºçš„è¿›åº¦
            sources = progress_data.get("sources") or {}
            for source, info in sources.items():
                fetched = info.get("fetched", 0)
                s_status = info.get("status", "unknown")
                print(f"  - {source}: {s_status}, å·²è·å– {fetched} ç¯‡")

            # ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥
            if status == "completed":
                print("\nâœ… ä»»åŠ¡å®Œæˆï¼")
                # è¿™é‡Œè¿”å›æ•´ä¸ª dataï¼Œè®©å¤–å±‚ç”¨ data['results'] è®¿é—®
                return data

            elif status == "failed":
                error = data.get("error", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"ä»»åŠ¡å¤±è´¥: {error}")


async def main():
    query = "agentic image restorationç›¸å…³çš„è®ºæ–‡ï¼ŒæŒ‰ç›¸å…³æ€§æ’åºï¼Œè¿”å›20ç¯‡"

    try:
        result = await search_papers_async(query, poll_interval=1.0)

        # å‡è®¾åç«¯è¿”å›ç»“æ„ï¼š{"status": "...", "progress": {...}, "results": [...]}
        papers = result.get("results", []).get("results", [])
        print(f"\næ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

        for i, paper in enumerate(papers, 1):
            title = paper.get("title", "æ— æ ‡é¢˜")
            authors = paper.get("authors") or []
            year = paper.get("year", "æœªçŸ¥å¹´ä»½")
            citations = paper.get("citations", 0)
            pdf_url = paper.get("pdf_url")

            print(f"\n{i}. {title}")
            if authors:
                print(f"   ä½œè€…: {', '.join(authors[:3])}")
            print(f"   å¹´ä»½: {year} | å¼•ç”¨: {citations}")
            print(f"   PDF: {pdf_url}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    # åœ¨æ™®é€š .py è„šæœ¬é‡Œè¿™æ ·è¿è¡Œå¼‚æ­¥ main
    asyncio.run(main())

```

### Python - æ‰¹é‡å¹¶å‘æŸ¥è¯¢ï¼ˆå¼‚æ­¥æ¥å£ï¼‰

```python
import asyncio
import httpx

async def create_task(client: httpx.AsyncClient, query: str) -> str:
    """åˆ›å»ºæœç´¢ä»»åŠ¡"""
    response = await client.post(
        "http://localhost:8000/tasks/search",
        json={"user_query": query}
    )
    return response.json()["task_id"]

async def wait_for_task(client: httpx.AsyncClient, task_id: str) -> dict:
    """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
    while True:
        response = await client.get(f"http://localhost:8000/tasks/{task_id}")
        data = response.json()

        if data["status"] in ["completed", "failed"]:
            return data

        await asyncio.sleep(2)

async def batch_search_async(queries: list[str]):
    """æ‰¹é‡å¹¶å‘æŸ¥è¯¢"""
    async with httpx.AsyncClient(timeout=120.0) as client:
        # 1. å¹¶å‘åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        print("åˆ›å»ºä»»åŠ¡...")
        task_ids = await asyncio.gather(*[
            create_task(client, query) for query in queries
        ])

        print(f"å·²åˆ›å»º {len(task_ids)} ä¸ªä»»åŠ¡\n")

        # 2. å¹¶å‘ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print("ç­‰å¾…ä»»åŠ¡å®Œæˆ...")
        results = await asyncio.gather(*[
            wait_for_task(client, task_id) for task_id in task_ids
        ])

        return results

# ä½¿ç”¨ç¤ºä¾‹
queries = [
    "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRï¼Œ2020-2023",
    "å¼ºåŒ–å­¦ä¹ æœºå™¨äººæ§åˆ¶ï¼ŒICLRï¼Œ2024",
    "Transformeræ¶æ„æœ€æ–°ç ”ç©¶ï¼ŒNeurIPSï¼Œ2023-2024",
]

results = await batch_search_async(queries)

for i, result in enumerate(results, 1):
    if result["status"] == "completed":
        count = len(result["results"]["results"])
        print(f"âœ… æŸ¥è¯¢ {i}: æ‰¾åˆ° {count} ç¯‡è®ºæ–‡")
    else:
        print(f"âŒ æŸ¥è¯¢ {i}: å¤±è´¥")
```

### JavaScript - åŒæ­¥æ¥å£

```javascript
async function searchPapers(query) {
  const url = new URL('http://localhost:8000/search');
  url.searchParams.append('user_query', query);

  const response = await fetch(url);
  const data = await response.json();
  return data;
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await searchPapers('å›¾ç¥ç»ç½‘ç»œåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨');

console.log(`æ‰¾åˆ° ${result.results.length} ç¯‡è®ºæ–‡`);
result.results.forEach(paper => {
  console.log(`æ ‡é¢˜: ${paper.title}`);
  console.log(`å¹´ä»½: ${paper.year} | å¼•ç”¨: ${paper.citations}\n`);
});
```

### JavaScript - å¼‚æ­¥æ¥å£ï¼ˆReact ç¤ºä¾‹ï¼‰

```javascript
import { useState, useEffect } from 'react';

function PaperSearchWithProgress({ query }) {
  const [taskId, setTaskId] = useState(null);
  const [status, setStatus] = useState('idle');
  const [progress, setProgress] = useState(0);
  const [stageDescription, setStageDescription] = useState('');
  const [results, setResults] = useState(null);
  const [error, setError] = useState(null);

  // åˆ›å»ºæœç´¢ä»»åŠ¡
  const startSearch = async () => {
    try {
      setStatus('creating');

      const response = await fetch('http://localhost:8000/tasks/search', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ user_query: query })
      });

      const data = await response.json();
      setTaskId(data.task_id);
      setStatus('polling');

    } catch (err) {
      setError(err.message);
      setStatus('error');
    }
  };

  // è½®è¯¢ä»»åŠ¡çŠ¶æ€
  useEffect(() => {
    if (!taskId || status !== 'polling') return;

    const pollInterval = setInterval(async () => {
      try {
        const response = await fetch(
          `http://localhost:8000/tasks/${taskId}`
        );
        const data = await response.json();

        setProgress(data.progress.overall_percent);
        setStageDescription(data.progress.stage_description || '');

        if (data.status === 'completed') {
          setResults(data.results);
          setStatus('completed');
          clearInterval(pollInterval);
        } else if (data.status === 'failed') {
          setError(data.error);
          setStatus('error');
          clearInterval(pollInterval);
        }

      } catch (err) {
        setError(err.message);
        setStatus('error');
        clearInterval(pollInterval);
      }
    }, 2000); // æ¯ 2 ç§’è½®è¯¢ä¸€æ¬¡

    return () => clearInterval(pollInterval);
  }, [taskId, status]);

  return (
    <div>
      {status === 'idle' && (
        <button onClick={startSearch}>å¼€å§‹æœç´¢</button>
      )}

      {status === 'polling' && (
        <div>
          <p>{stageDescription} - {progress}%</p>
          <progress value={progress} max={100} />
        </div>
      )}

      {status === 'completed' && (
        <div>
          <h3>æ‰¾åˆ° {results.results.length} ç¯‡è®ºæ–‡</h3>
          {results.results.map((paper, i) => (
            <div key={i}>
              <h4>{paper.title}</h4>
              <p>å¹´ä»½: {paper.year} | å¼•ç”¨: {paper.citations}</p>
            </div>
          ))}
        </div>
      )}

      {status === 'error' && (
        <p style={{color: 'red'}}>é”™è¯¯: {error}</p>
      )}
    </div>
  );
}
```

---

## æŸ¥è¯¢è¯­æ³•

ç³»ç»Ÿæ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ŒLLM ä¼šè‡ªåŠ¨è§£æä»¥ä¸‹ä¿¡æ¯ï¼š

### æ”¯æŒçš„æŸ¥è¯¢è¦ç´ 

| è¦ç´  | ç¤ºä¾‹ | è¯´æ˜ |
|------|------|------|
| **ä¸»é¢˜å…³é”®è¯** | "æ·±åº¦å­¦ä¹ "ã€"ç›®æ ‡æ£€æµ‹" | è‡ªåŠ¨æå–å¹¶ç»„åˆ |
| **æ—¶é—´èŒƒå›´** | "2020åˆ°2023"ã€"æœ€è¿‘ä¸‰å¹´" | è‡ªåŠ¨è½¬æ¢ä¸ºæ—¥æœŸ |
| **ä¼šè®®/æœŸåˆŠ** | "CVPR"ã€"NeurIPSæˆ–ICLR" | æ”¯æŒå¤šä¸ªåœºé¦† |
| **æ–‡çŒ®ç±»å‹** | "ç»¼è¿°"ã€"Review" | è‡ªåŠ¨è¯†åˆ«ç±»å‹ |
| **PDFè¦æ±‚** | "è¦æ±‚æœ‰PDF"ã€"å¿…é¡»å¼€æº" | è¿‡æ»¤æ¡ä»¶ |
| **æ’åºæ–¹å¼** | "æŒ‰å¼•ç”¨æ•°æ’åº"ã€"æŒ‰æ—¶é—´æ’åº" | è‡ªåŠ¨è¯†åˆ« |
| **å¼•ç”¨æ•°é™åˆ¶** | "å¼•ç”¨æ•°å¤§äº50" | è¿‡æ»¤ä½å¼•ç”¨è®ºæ–‡ |

### æŸ¥è¯¢ç¤ºä¾‹

```
âœ… "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRä¼šè®®ï¼Œ2020åˆ°2023å¹´"
âœ… "å¼ºåŒ–å­¦ä¹ ä¸æœºå™¨äººæ§åˆ¶ï¼ŒICLRæˆ–NeurIPSï¼ŒæŒ‰å¼•ç”¨æ•°æ’åº"
âœ… "Transformeræ¶æ„çš„æœ€æ–°ç ”ç©¶ï¼Œè¦æ±‚æœ‰PDFï¼Œæœ€è¿‘ä¸¤å¹´"
âœ… "å¤šæ¨¡æ€å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒä¸­çš„åº”ç”¨ï¼Œç»¼è¿°ç±»æ–‡ç« "
âœ… "å›¾ç¥ç»ç½‘ç»œæ¨èç³»ç»Ÿï¼Œå¼•ç”¨æ•°å¤§äº100ï¼Œ2023-2024"
âœ… "ææ–™ç§‘å­¦ä¸­çŸ³å¢¨çƒ¯çš„å‚¨èƒ½åº”ç”¨ç»¼è¿°ï¼Œæœ€è¿‘äº”å¹´"
```

---

## è¿”å›ç»“æœ

### å“åº”ç»“æ„

```json
{
  "query": "ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢",
  "normalized_intent": { },
  "api_params": { },
  "counts": { },
  "results": [ ]
}
```

### å®Œæ•´å“åº”ç¤ºä¾‹

```json
{
  "task_id": "59eecf97-09f5-4840-9727-196d7a2deaa6",
  "status": "completed",
  "progress": {
    "stage": "completed",
    "stage_description": "è¿›å…¥æ’åºé˜¶æ®µ",
    "sources": {
      "s2": {
        "status": "completed",
        "fetched": 1,
        "total_estimated": null,
        "errors": null
      },
      "openalex": {
        "status": "completed",
        "fetched": 1,
        "total_estimated": null,
        "errors": null
      },
      "arxiv": {
        "status": "completed",
        "fetched": 50,
        "total_estimated": null,
        "errors": null
      }
    },
    "overall_percent": 100
  },
  "created_at": "2025-11-16T07:05:42.155394",
  "updated_at": "2025-11-16T07:05:46.292991",
  "results": {
    "query": "agentic image restorationç›¸å…³çš„è®ºæ–‡ï¼ŒæŒ‰ç›¸å…³æ€§æ’åº",
    "normalized_intent": {
      "any_groups": [
        [
          "agentic image restoration"
        ]
      ],
      "enabled_sources": [
        "s2",
        "openalex",
        "arxiv"
      ],
      "venues": [],
      "author": null,
      "date_start": null,
      "date_end": null,
      "must_have_pdf": false,
      "publication_types": [],
      "open_access": null,
      "min_influential_citations": null,
      "min_citations": null,
      "max_results": 10,
      "sort_by": "relevance",
      "language": null
    },
    "api_params": {
      "endpoint": "graph/v1/paper/search/bulk",
      "query_combinations": 1,
      "queries": [
        "[s2] \"agentic image restoration\"",
        "[openalex] \"agentic image restoration\"",
        "[arxiv] \"agentic image restoration\""
      ]
    },
    "counts": {
      "query_combinations": 1,
      "total_raw_fetched": 52,
      "total_raw_unique": 52,
      "final_unique_count": 51,
      "after_rank_cut": 10
    },
    "results": [
      {
        "title": "MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow",
        "authors": [
          "Kihyun Na",
          "Junseok Oh",
          "Youngkwan Cho",
          "Bumjin Kim",
          "Sungmin Cho",
          "Jinyoung Choi",
          "Injung Kim"
        ],
        "first_author_hindex": null,
        "abstract": "License plate recognition (LPR) is important for traffic law enforcement, crime investigation, and surveillance. However, license plate areas in dash cam images often suffer from low resolution, motion blur, and glare, which make accurate recognition challenging. Existing generative models that rely on pretrained priors cannot reliably restore such poor-quality images, frequently introducing severe artifacts and distortions. To address this issue, we propose a novel multi-frame license plate restoration and recognition framework, MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and aggregating neighboring frames instead of relying on pretrained knowledge. To achieve accurate frame alignment, we employ a state-of-the-art optical flow estimator in conjunction with carefully designed algorithms that detect and correct erroneous optical flow estimations by leveraging the spatio-temporal consistency inherent in license plate image sequences. Our approach enhances both image quality and recognition accuracy while preserving the evidential content of the input images. In addition, we constructed a novel Realistic LPR (RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of low-quality license plate image sequences and high-quality pseudo ground-truth images, reflecting the complexities of real-world scenarios. In experiments, MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM, and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and the multi-frame LPR (82.55%) among the eleven baseline models. The results of ablation studies confirm that our filtering and refinement algorithms significantly contribute to these improvements.",
        "year": 2025,
        "doi": "10.1016/j.cviu.2025.104361",
        "journal": "arXiv",
        "url": "http://arxiv.org/abs/2508.14797v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14797v1.pdf",
        "citations": null,
        "influential_citations": null,
        "open_access": true,
        "publication_types": [
          "preprint"
        ],
        "publication_date": "2025-08-19",
        "fields_of_study": []
      }
    ]
  },
  "completed_at": "2025-11-16T07:05:46.292991"
}
```

### å…³é”®å­—æ®µè¯´æ˜

#### 1. `normalized_intent` - LLM è§£æçš„æŸ¥è¯¢æ„å›¾

| å­—æ®µ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `any_groups` | å…³é”®è¯ç»„åˆï¼ˆAND-of-ORï¼‰ | `[["deep learning"], ["object detection"]]` |
| `enabled_sources` | ä½¿ç”¨çš„æ•°æ®æº | `["s2", "openalex", "arxiv"]` |
| `venues` | ç›®æ ‡ä¼šè®®/æœŸåˆŠ | `["CVPR", "ICCV"]` |
| `date_start` / `date_end` | æ—¶é—´èŒƒå›´ | `"2022-01-01"` ~ `"2025-11-15"` |
| `publication_types` | æ–‡çŒ®ç±»å‹ | `["Review", "Conference"]` |
| `sort_by` | æ’åºæ–¹å¼ | `"publicationDate"` / `"citationCount"` / `"relevance"` |
| `max_results` | æœ€å¤§è¿”å›æ•° | `10` |

#### 2. `counts` - æ£€ç´¢ç»Ÿè®¡

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `query_combinations` | ç”Ÿæˆçš„æŸ¥è¯¢ç»„åˆæ•° |
| `total_raw_fetched` | ä»æ‰€æœ‰æ•°æ®æºæŠ“å–çš„åŸå§‹è®ºæ–‡æ•° |
| `total_raw_unique` | åˆæ­¥å»é‡åçš„è®ºæ–‡æ•° |
| `final_unique_count` | è¿‡æ»¤åçš„å”¯ä¸€è®ºæ–‡æ•° |
| `after_rank_cut` | æœ€ç»ˆè¿”å›çš„è®ºæ–‡æ•° |

#### 3. `results` - è®ºæ–‡åˆ—è¡¨

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `title` | string | è®ºæ–‡æ ‡é¢˜ |
| `authors` | string[] | ä½œè€…åˆ—è¡¨ |
| `first_author_hindex` | int | ç¬¬ä¸€ä½œè€… H-index (å¯èƒ½ä¸º null) |
| `abstract` | string | æ‘˜è¦ |
| `year` | int | å‘è¡¨å¹´ä»½ |
| `journal` | string | æœŸåˆŠ/ä¼šè®®åç§° |
| `url` | string | è®ºæ–‡ä¸»é¡µé“¾æ¥ |
| `pdf_url` | string | PDF ç›´æ¥ä¸‹è½½é“¾æ¥ (å¯èƒ½ä¸º null) |
| `citations` | int | å¼•ç”¨æ•° |
| `influential_citations` | int | å½±å“åŠ›å¼•ç”¨æ•° |
| `open_access` | boolean | æ˜¯å¦æœ‰å¼€æ”¾ PDF |
| `publication_date` | string | å‘è¡¨æ—¥æœŸ (YYYY-MM-DD) |
| `fields_of_study` | string[] | ç ”ç©¶é¢†åŸŸ |

---

## å®é™…æ•ˆæœæ¼”ç¤º

### ç¤ºä¾‹ 1: è®¡ç®—æœºè§†è§‰ç»¼è¿°

**æŸ¥è¯¢**:
```
æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRä¼šè®®ï¼Œæœ€è¿‘ä¸‰å¹´
```

**æ•ˆæœ**:
- è‡ªåŠ¨è¯†åˆ«å…³é”®è¯: "deep learning", "object detection", "survey"
- é™å®šä¼šè®®: CVPR
- æ—¶é—´èŒƒå›´: 2022-01-01 ~ 2025-11-15
- æ–‡çŒ®ç±»å‹: Review
- æ•°æ®æº: S2, OpenAlex, arXiv
- è¿”å›: 10 ç¯‡ç›¸å…³ç»¼è¿°è®ºæ–‡

**å…¸å‹è¿”å›**:
```json
{
  "results": [
    {
      "title": "Deep Learning for Object Detection: A Comprehensive Survey",
      "year": 2024,
      "journal": "CVPR",
      "citations": 245,
      "open_access": true
    }
  ]
}
```

---

### ç¤ºä¾‹ 2: å¼ºåŒ–å­¦ä¹ é¡¶ä¼šè®ºæ–‡

**æŸ¥è¯¢**:
```
å¼ºåŒ–å­¦ä¹ ä¸æœºå™¨äººæ§åˆ¶ï¼ŒICLRæˆ–NeurIPSï¼Œ2024å¹´ï¼ŒæŒ‰å¼•ç”¨æ•°æ’åº
```

**æ•ˆæœ**:
- å…³é”®è¯: "reinforcement learning", "robot control"
- ä¼šè®®: ICLR, NeurIPS
- æ—¶é—´: 2024-01-01 ~ 2024-12-31
- æ’åº: å¼•ç”¨æ•°é™åº
- è¿”å›: 10 ç¯‡é«˜å¼•ç”¨è®ºæ–‡

**å…¸å‹è¿”å›**:
```json
{
  "results": [
    {
      "title": "Hierarchical Reinforcement Learning for Robot Control",
      "year": 2024,
      "journal": "ICLR",
      "citations": 89,
      "influential_citations": 12
    }
  ]
}
```

---

### ç¤ºä¾‹ 3: ç”Ÿç‰©åŒ»å­¦å¼€æ”¾è®ºæ–‡

**æŸ¥è¯¢**:
```
è›‹ç™½è´¨ç»“æ„é¢„æµ‹çš„æœ€æ–°è¿›å±•ï¼Œè¦æ±‚æœ‰PDFï¼Œæœ€è¿‘ä¸¤å¹´
```

**æ•ˆæœ**:
- å…³é”®è¯: "protein structure prediction"
- æ—¶é—´: 2023-01-01 ~ 2025-11-15
- è¿‡æ»¤: å¿…é¡»æœ‰å¼€æ”¾ PDF
- æ•°æ®æº: S2, PubMed, Europe PMC
- è¿”å›: 10 ç¯‡å¼€æ”¾è·å–è®ºæ–‡

**å…¸å‹è¿”å›**:
```json
{
  "results": [
    {
      "title": "AlphaFold 3: Advances in Protein Structure Prediction",
      "year": 2024,
      "journal": "Nature",
      "citations": 1250,
      "open_access": true,
      "url": "https://..."
    }
  ]
}
```

---

### ç¤ºä¾‹ 4: è·¨å­¦ç§‘ææ–™ç ”ç©¶

**æŸ¥è¯¢**:
```
ææ–™ç§‘å­¦ä¸­çŸ³å¢¨çƒ¯çš„å‚¨èƒ½åº”ç”¨ç»¼è¿°ï¼Œæœ€è¿‘äº”å¹´
```

**æ•ˆæœ**:
- å…³é”®è¯: "graphene", "energy storage", "materials science", "survey"
- æ—¶é—´: 2020-01-01 ~ 2025-11-15
- æ–‡çŒ®ç±»å‹: Review
- æ•°æ®æº: S2, OpenAlex
- è¿”å›: 10 ç¯‡ç»¼è¿°è®ºæ–‡

**å…¸å‹è¿”å›**:
```json
{
  "results": [
    {
      "title": "Graphene-based Materials for Energy Storage: A Review",
      "year": 2023,
      "journal": "Advanced Materials",
      "citations": 342,
      "fields_of_study": ["Materials Science", "Chemistry"]
    }
  ]
}
```

---

## æ•°æ®æºè¯´æ˜

ç³»ç»Ÿè‡ªåŠ¨ä»å¤šä¸ªå­¦æœ¯æ•°æ®åº“æ£€ç´¢ï¼Œå¹¶æ™ºèƒ½å»é‡ï¼š

| æ•°æ®æº | è¦†ç›–é¢†åŸŸ | ç‰¹ç‚¹ |
|--------|----------|------|
| **Semantic Scholar** | å…¨å­¦ç§‘ | å¿…é€‰ï¼Œæœ€å…¨é¢ |
| **OpenAlex** | å…¨å­¦ç§‘ | å¼€æ”¾æ•°æ®ï¼Œæ›´æ–°å¿« |
| **arXiv** | ç‰©ç†/è®¡ç®—æœº/æ•°å­¦ | é¢„å°æœ¬ï¼Œæœ€æ–°ç ”ç©¶ |
| **PubMed** | ç”Ÿç‰©åŒ»å­¦ | æƒå¨åŒ»å­¦æ–‡çŒ® |
| **Europe PMC** | ç”Ÿå‘½ç§‘å­¦ | æ¬§æ´²ç”Ÿå‘½ç§‘å­¦æ•°æ® |
| **Crossref** | å…¨å­¦ç§‘ | DOI å…ƒæ•°æ® |

LLM ä¼šæ ¹æ®æŸ¥è¯¢ä¸»é¢˜è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®æºç»„åˆã€‚

---

## é”™è¯¯å¤„ç†

### é”™è¯¯å“åº”æ ¼å¼

```json
{
  "query": "ç”¨æˆ·æŸ¥è¯¢",
  "error": "é”™è¯¯ä¿¡æ¯",
  "results": [],
  "counts": {
    "query_combinations": 0,
    "total_raw_fetched": 0,
    "total_raw_unique": 0,
    "final_unique_count": 0,
    "after_rank_cut": 0
  }
}
```

### å¸¸è§é”™è¯¯

**1. æŸ¥è¯¢è¿‡å®½**
```json
{
  "error": "S2 API error: 400 too many hits"
}
```
**è§£å†³**: æ·»åŠ æ›´å…·ä½“çš„å…³é”®è¯ã€é™åˆ¶æ—¶é—´èŒƒå›´æˆ–æŒ‡å®šä¼šè®®

**2. LLM è§£æå¤±è´¥**
```json
{
  "error": "LLM parsing failed: Invalid JSON response"
}
```
**è§£å†³**: æ£€æŸ¥ API Key æˆ–ç®€åŒ–æŸ¥è¯¢è¯­å¥

**3. æ— ç»“æœ**
```json
{
  "results": [],
  "counts": { "final_unique_count": 0 }
}
```
**è§£å†³**: æ”¾å®½è¿‡æ»¤æ¡ä»¶æˆ–æ‰©å¤§æ—¶é—´èŒƒå›´

---

## æ‰¹é‡è°ƒç”¨ç¤ºä¾‹

### Python æ‰¹é‡æŸ¥è¯¢

```python
import asyncio
import httpx

async def batch_search(queries: list[str]):
    """æ‰¹é‡æŸ¥è¯¢è®ºæ–‡"""
    async with httpx.AsyncClient(timeout=60.0) as client:
        tasks = [
            client.get("http://localhost:8000/search", params={"user_query": q})
            for q in queries
        ]
        responses = await asyncio.gather(*tasks)
        return [r.json() for r in responses]

# ä½¿ç”¨ç¤ºä¾‹
queries = [
    "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRï¼Œ2020-2023",
    "å¼ºåŒ–å­¦ä¹ æœºå™¨äººæ§åˆ¶ï¼ŒICLRï¼Œ2024",
    "Transformeræ¶æ„æœ€æ–°ç ”ç©¶ï¼ŒNeurIPSï¼Œ2023-2024",
]

results = await batch_search(queries)

for i, result in enumerate(results, 1):
    print(f"\næŸ¥è¯¢ {i}: {result['query']}")
    print(f"æ‰¾åˆ° {len(result['results'])} ç¯‡è®ºæ–‡")
    if result['results']:
        top_paper = result['results'][0]
        print(f"Top 1: {top_paper['title']}")
```

### å¸¦å»¶è¿Ÿçš„æ‰¹é‡æŸ¥è¯¢ï¼ˆé¿å…é€Ÿç‡é™åˆ¶ï¼‰

```python
async def batch_search_with_delay(queries: list[str], delay: float = 2.0):
    """æ‰¹é‡æŸ¥è¯¢ï¼Œæ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶"""
    results = []
    async with httpx.AsyncClient(timeout=60.0) as client:
        for query in queries:
            response = await client.get(
                "http://localhost:8000/search",
                params={"user_query": query}
            )
            results.append(response.json())
            await asyncio.sleep(delay)  # æ¯æ¬¡æŸ¥è¯¢é—´éš” 2 ç§’
    return results
```

---

## å¸¸è§é—®é¢˜

### Q1: åŒæ­¥æ¥å£ vs å¼‚æ­¥æ¥å£ï¼Œå¦‚ä½•é€‰æ‹©ï¼Ÿ

**ä½¿ç”¨åŒæ­¥æ¥å£** (`GET /search`) çš„æƒ…å†µï¼š
- âœ… ç®€å•å¿«é€ŸæŸ¥è¯¢ï¼ˆé¢„è®¡ <5 ç§’ï¼‰
- âœ… å‘½ä»¤è¡Œå·¥å…·ã€è„šæœ¬
- âœ… ä¸éœ€è¦æ˜¾ç¤ºè¿›åº¦çš„åœºæ™¯

**ä½¿ç”¨å¼‚æ­¥æ¥å£** (`POST /tasks/search`) çš„æƒ…å†µï¼š
- âœ… å¤æ‚æŸ¥è¯¢ï¼ˆå¤šæ¥æºã€å¤šå…³é”®è¯ï¼‰
- âœ… éœ€è¦æ˜¾ç¤ºå®æ—¶è¿›åº¦çš„å‰ç«¯ç•Œé¢
- âœ… é¿å… HTTP è¶…æ—¶é—®é¢˜
- âœ… éœ€è¦å¹¶å‘å¤„ç†å¤šä¸ªæŸ¥è¯¢

### Q2: å¦‚ä½•è·å–æ›´å¤šç»“æœï¼Ÿ

**æ–¹æ³• 1: åœ¨æŸ¥è¯¢ä¸­æŒ‡å®šæ•°é‡**ï¼ˆæ¨èï¼‰
```
"æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ï¼Œè¿”å›50ç¯‡è®ºæ–‡"
"å¼ºåŒ–å­¦ä¹ ç»¼è¿°ï¼Œéœ€è¦100ç¯‡"
"Transformeræ¶æ„ç ”ç©¶ï¼Œæœ€å¤š200ç¯‡"
```

LLM ä¼šè‡ªåŠ¨è¯†åˆ«æ•°é‡è¦æ±‚å¹¶è®¾ç½® `max_results`ã€‚

**æ–¹æ³• 2: é…ç½®æœåŠ¡å™¨é™åˆ¶**

é»˜è®¤æœ€å¤§é™åˆ¶ä¸º **500 ç¯‡è®ºæ–‡**ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´ï¼š
```bash
# .env æ–‡ä»¶
MAX_RESULTS_LIMIT=500  # æœ€å¤§é™åˆ¶ï¼ˆ1-1000ï¼‰
```

**æ³¨æ„äº‹é¡¹**:
- é»˜è®¤è¿”å› 10 ç¯‡ï¼ˆæœªæŒ‡å®šæ—¶ï¼‰
- æ¨èä½¿ç”¨å¼‚æ­¥æ¥å£ (`/tasks/search`) è¯·æ±‚ 100+ ç¯‡è®ºæ–‡
- è¶…è¿‡é…ç½®é™åˆ¶ä¼šè¿”å› HTTP 400 é”™è¯¯
- æ›´å¤§çš„ç»“æœé›†éœ€è¦æ›´é•¿çš„å“åº”æ—¶é—´ï¼ˆ200ç¯‡çº¦ 4-10sï¼Œ500ç¯‡çº¦ 10-25sï¼‰

### Q3: å¼‚æ­¥æ¥å£çš„è½®è¯¢é¢‘ç‡å»ºè®®ï¼Ÿ

**æ¨èè½®è¯¢é—´éš”**:
- æ ‡å‡†æŸ¥è¯¢: æ¯ **1-2 ç§’** è½®è¯¢ä¸€æ¬¡
- å¤æ‚æŸ¥è¯¢: æ¯ **2-3 ç§’** è½®è¯¢ä¸€æ¬¡
- é¿å…: <1 ç§’çš„é«˜é¢‘è½®è¯¢ï¼ˆæµªè´¹èµ„æºï¼‰

**æœ€ä½³å®è·µ**:
```javascript
// âœ… å¥½çš„åšæ³•ï¼šæŒ‡æ•°é€€é¿
let interval = 1000;
const poll = async () => {
  const data = await fetchTaskStatus(taskId);
  if (data.status === 'completed') return data;

  interval = Math.min(interval * 1.2, 5000); // æœ€å¤š 5 ç§’
  await sleep(interval);
  return poll();
};

// âŒ ä¸å¥½çš„åšæ³•ï¼šå›ºå®šé«˜é¢‘è½®è¯¢
setInterval(() => fetchTaskStatus(taskId), 500); // å¤ªé¢‘ç¹ï¼
```

### Q4: ä»»åŠ¡ä¼šè‡ªåŠ¨æ¸…ç†å—ï¼Ÿ

æ˜¯çš„ã€‚ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥åï¼Œä¼šåœ¨ **30 åˆ†é’Ÿåè‡ªåŠ¨æ¸…ç†**ã€‚

- âœ… èŠ‚çœæœåŠ¡å™¨å†…å­˜
- âš ï¸ è·å–ç»“æœååº”ç«‹å³ä¿å­˜
- ğŸ“ ä»»åŠ¡ ID æ— æ³•æ¢å¤å·²æ¸…ç†çš„ä»»åŠ¡

### Q5: å¦‚æœä»»åŠ¡å¤±è´¥äº†æ€ä¹ˆåŠï¼Ÿ

æ£€æŸ¥è¿”å›çš„ `error` å­—æ®µï¼š
```json
{
  "status": "failed",
  "error": "Failed to parse query: invalid syntax"
}
```

**å¸¸è§å¤±è´¥åŸå› **:
- LLM API é…ç½®é”™è¯¯ï¼ˆæ£€æŸ¥ API Keyï¼‰
- æŸ¥è¯¢è¿‡äºå¤æ‚æˆ–æ¨¡ç³Š
- ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
1. ç®€åŒ–æŸ¥è¯¢è¯­å¥
2. æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
3. æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—

### Q6: æ”¯æŒå“ªäº›ä¼šè®®/æœŸåˆŠï¼Ÿ

æ”¯æŒæ‰€æœ‰ä¸»æµä¼šè®®/æœŸåˆŠï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
- **AI/ML**: NeurIPS, ICLR, ICML, AAAI, IJCAI
- **CV**: CVPR, ICCV, ECCV
- **NLP**: ACL, EMNLP, NAACL
- **Data**: KDD, WWW, SIGIR

### Q7: å¦‚ä½•è·å–è®ºæ–‡å…¨æ–‡ï¼Ÿ

API åªè¿”å›å…ƒæ•°æ®ã€‚è·å–å…¨æ–‡ï¼š
- å¼€æ”¾è·å–è®ºæ–‡ (`open_access: true`): è®¿é—® `url` å­—æ®µ
- éå¼€æ”¾è®ºæ–‡: é€šè¿‡ DOI æˆ–æœºæ„è®¢é˜…è·å–
- arXiv è®ºæ–‡: å°† URL ä¸­çš„ `/abs/` æ”¹ä¸º `/pdf/`

### Q8: ä¸­æ–‡æŸ¥è¯¢æ•ˆæœå¦‚ä½•ï¼Ÿ

å®Œå…¨æ”¯æŒä¸­æ–‡æŸ¥è¯¢ï¼ŒLLM ä¼šè‡ªåŠ¨ï¼š
- æå–ä¸­æ–‡å…³é”®è¯
- ç¿»è¯‘ä¸ºè‹±æ–‡è¿›è¡Œæ£€ç´¢
- è¯†åˆ«ä¸­æ–‡è¡¨è¿°çš„æ—¶é—´ã€ä¼šè®®ç­‰

### Q9: è¿”å›ç»“æœæ˜¯å®æ—¶çš„å—ï¼Ÿ

è¿”å›ç»“æœåŸºäºå„æ•°æ®æºçš„æœ€æ–°ç´¢å¼•ï¼Œé€šå¸¸ï¼š
- arXiv: æ¯æ—¥æ›´æ–°
- Semantic Scholar: æ¯å‘¨æ›´æ–°
- PubMed: æ¯æ—¥æ›´æ–°
- OpenAlex: æŒç»­æ›´æ–°

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡æŸ¥è¯¢æ—¶ä½¿ç”¨å¼‚æ­¥æ¥å£

```python
# âŒ ä¸å¥½ï¼šä¸²è¡ŒåŒæ­¥æŸ¥è¯¢ï¼ˆæ…¢ï¼‰
for query in queries:
    result = await search_sync(query)  # æ¯ä¸ªç­‰ 10 ç§’

# âœ… å¥½ï¼šå¹¶å‘å¼‚æ­¥æŸ¥è¯¢ï¼ˆå¿«ï¼‰
task_ids = await asyncio.gather(*[
    create_task(query) for query in queries
])
results = await asyncio.gather(*[
    wait_for_task(tid) for tid in task_ids
])
```

### 2. åˆç†è®¾ç½®è¶…æ—¶

```python
# åŒæ­¥æ¥å£
timeout = 60.0  # å¤æ‚æŸ¥è¯¢å¯èƒ½éœ€è¦ 30-60 ç§’

# å¼‚æ­¥æ¥å£
timeout = 120.0  # è½®è¯¢è¶…æ—¶å¯ä»¥æ›´é•¿
```

### 3. ç¼“å­˜ç»“æœ

å¯¹äºç›¸åŒæŸ¥è¯¢ï¼Œå»ºè®®åœ¨å®¢æˆ·ç«¯ç¼“å­˜ç»“æœï¼š

```python
import hashlib
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_results(query_hash: str):
    # ç¼“å­˜æœ€è¿‘ 100 ä¸ªæŸ¥è¯¢ç»“æœ
    pass
```

### 4. é”™è¯¯é‡è¯•

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def search_with_retry(query: str):
    return await search_papers_async(query)
```

---

## è¿›åº¦è·Ÿè¸ªç³»ç»Ÿè¯¦è§£

### è¿›åº¦è®¡ç®—æ–¹å¼

å¼‚æ­¥ä»»åŠ¡çš„è¿›åº¦åŸºäº**å›ºå®šé˜¶æ®µå¹³å‡åˆ†é…**åŸåˆ™ï¼Œç¡®ä¿è¿›åº¦å¢é•¿çš„ä¸€è‡´æ€§å’Œå¯é¢„æµ‹æ€§ã€‚

#### é˜¶æ®µåˆ†é…ï¼ˆ4ä¸ªä¸»è¦é˜¶æ®µï¼‰

| é˜¶æ®µ | è¿›åº¦èŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| **åˆ›å»ºä»»åŠ¡** | 0% | ä»»åŠ¡åˆå§‹åŒ–å®Œæˆ |
| **è§£ææ„å›¾** | 0% â†’ 25% | LLM è§£ææŸ¥è¯¢ï¼Œæå–å…³é”®è¯å’Œè¿‡æ»¤æ¡ä»¶ |
| **æœç´¢æ–‡çŒ®** | 25% â†’ 75% | å¤šæ¥æºå¹¶å‘æ£€ç´¢ï¼ˆåŠ¨æ€è®¡ç®—ï¼‰ |
| **æ’åºç»“æœ** | 75% â†’ 100% | å»é‡ã€è¿‡æ»¤ã€æ’åº |

#### æœç´¢é˜¶æ®µåŠ¨æ€è®¡ç®—

åœ¨æœç´¢é˜¶æ®µï¼ˆå æ€»è¿›åº¦çš„ 50%ï¼‰ï¼Œè¿›åº¦æ ¹æ®å®Œæˆçš„æ¥æºæ•°åŠ¨æ€è®¡ç®—ï¼š

```
è¿›åº¦ = 25% + 50% Ã— (å·²å®Œæˆæ¥æºæ•° / æ€»æ¥æºæ•°)
```

**ç¤ºä¾‹**ï¼ˆ3ä¸ªæ¥æºï¼šs2, openalex, arxivï¼‰ï¼š

```
é˜¶æ®µ              è¿›åº¦è®¡ç®—                          è¿›åº¦å€¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä»»åŠ¡åˆ›å»º          å›ºå®š                              0%
è§£æå®Œæˆ          å›ºå®š                              25%
S2 å®Œæˆ           25% + 50% Ã— (1/3)                41%
OpenAlex å®Œæˆ     25% + 50% Ã— (2/3)                58%
arXiv å®Œæˆ        25% + 50% Ã— (3/3)                75%
æ’åºå®Œæˆ          å›ºå®š                              100%
```

### ä¸­æ–‡æè¿°ç³»ç»Ÿ

æ¯ä¸ªé˜¶æ®µå’Œæ¥æºçŠ¶æ€éƒ½æœ‰å¯¹åº”çš„**å‹å¥½ä¸­æ–‡æè¿°**ï¼Œä¾¿äºå‰ç«¯ç›´æ¥å±•ç¤ºï¼š

#### ä¸»é˜¶æ®µæè¿°

| é˜¶æ®µ | æè¿° |
|------|------|
| `created` | "ä»»åŠ¡å·²åˆ›å»º" |
| `parsing` | "æ­£åœ¨è§£ææŸ¥è¯¢æ„å›¾" |
| `searching` | "æ­£åœ¨æœç´¢ {æ¥æºåç§°}" (åŠ¨æ€) |
| `ranking` | "è¿›å…¥æ’åºé˜¶æ®µ" |
| `completed` | "æœç´¢å®Œæˆ" |
| `failed` | "æœç´¢å¤±è´¥" |

#### æ¥æºåç§°æ˜ å°„

| è‹±æ–‡æ ‡è¯† | ä¸­æ–‡åç§° |
|---------|---------|
| `s2` | Semantic Scholar |
| `openalex` | OpenAlex |
| `arxiv` | arXiv |
| `crossref` | Crossref |
| `pubmed` | PubMed |
| `eupmc` | Europe PMC |

#### æœç´¢é˜¶æ®µæè¿°é€»è¾‘

åœ¨æœç´¢é˜¶æ®µï¼Œ`stage_description` ä¼š**åŠ¨æ€æ˜¾ç¤ºå½“å‰æ­£åœ¨æœç´¢çš„æ¥æº**ï¼š

```json
// S2 æ­£åœ¨æœç´¢æ—¶
{"stage_description": "æ­£åœ¨æœç´¢ Semantic Scholar"}

// OpenAlex æ­£åœ¨æœç´¢æ—¶
{"stage_description": "æ­£åœ¨æœç´¢ OpenAlex"}

// æ‰€æœ‰æ¥æºå®Œæˆæ—¶
{"stage_description": "æ–‡çŒ®æ£€ç´¢å®Œæˆ"}
```

### å‰ç«¯é›†æˆç¤ºä¾‹

#### æ˜¾ç¤ºè¿›åº¦æ¡ + ä¸­æ–‡æè¿°

```javascript
// React ç¤ºä¾‹
<div className="search-progress">
  <h4>{progress.stage_description}</h4>
  <ProgressBar value={progress.overall_percent} max={100} />
  <p>{progress.overall_percent}% å®Œæˆ</p>
</div>
```

**æ•ˆæœ**ï¼š
```
æ­£åœ¨æœç´¢ OpenAlex
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 58% å®Œæˆ
```

#### æ˜¾ç¤ºè¯¦ç»†æ¥æºè¿›åº¦

```javascript
// æ˜¾ç¤ºæ¯ä¸ªæ¥æºçš„çŠ¶æ€
{Object.entries(progress.sources).map(([source, info]) => (
  <div key={source} className="source-item">
    <span className="source-name">{SOURCE_NAMES[source]}</span>
    <span className={`status-${info.status}`}>
      {STATUS_LABELS[info.status]}
    </span>
    {info.fetched && <span>({info.fetched} ç¯‡)</span>}
  </div>
))}
```

**æ•ˆæœ**ï¼š
```
âœ… Semantic Scholar  å·²å®Œæˆ (120 ç¯‡)
â³ OpenAlex         æ£€ç´¢ä¸­ (45 ç¯‡)
â¸ï¸  arXiv            ç­‰å¾…ä¸­
```

### è½®è¯¢æœ€ä½³å®è·µ

#### æ¨èè½®è¯¢ç­–ç•¥

```javascript
// âœ… å¥½çš„åšæ³•ï¼šè‡ªé€‚åº”è½®è¯¢é—´éš”
const pollWithBackoff = async (taskId) => {
  let interval = 1000;  // åˆå§‹ 1 ç§’
  let previousPercent = 0;

  while (true) {
    const data = await fetchTaskStatus(taskId);

    // æ£€æŸ¥æ˜¯å¦å®Œæˆ
    if (data.status === 'completed' || data.status === 'failed') {
      return data;
    }

    // å¦‚æœè¿›åº¦åœ¨å˜åŒ–ï¼Œä¿æŒå¿«é€Ÿè½®è¯¢
    if (data.progress.overall_percent > previousPercent) {
      interval = 1000;  // é‡ç½®ä¸º 1 ç§’
    } else {
      // è¿›åº¦åœæ»ï¼Œé€æ¸å¢åŠ é—´éš”
      interval = Math.min(interval * 1.2, 5000);  // æœ€å¤š 5 ç§’
    }

    previousPercent = data.progress.overall_percent;
    await sleep(interval);
  }
};
```

#### é¿å…çš„åšæ³•

```javascript
// âŒ ä¸å¥½çš„åšæ³•ï¼šå›ºå®šé«˜é¢‘è½®è¯¢
setInterval(() => fetchTaskStatus(taskId), 500);  // å¤ªé¢‘ç¹ï¼

// âŒ ä¸å¥½çš„åšæ³•ï¼šå¿½ç•¥è¿›åº¦ä¿¡æ¯
while (true) {
  await fetchTaskStatus(taskId);
  await sleep(10000);  // é—´éš”å¤ªé•¿ï¼Œä½“éªŒå·®
}
```

### è¿›åº¦å¼‚å¸¸å¤„ç†

#### è¿›åº¦å¡ä½

å¦‚æœè¿›åº¦é•¿æ—¶é—´ï¼ˆ>30ç§’ï¼‰åœç•™åœ¨æŸä¸ªç™¾åˆ†æ¯”ï¼š

```javascript
let stuckTimer = 0;
const MAX_STUCK_TIME = 30000;  // 30 ç§’

if (currentPercent === previousPercent) {
  stuckTimer += pollInterval;
  if (stuckTimer > MAX_STUCK_TIME) {
    console.warn('ä»»åŠ¡å¯èƒ½å¡ä½ï¼Œå»ºè®®åˆ·æ–°æˆ–é‡æ–°åˆ›å»º');
    // å¯é€‰ï¼šè‡ªåŠ¨é‡è¯•æˆ–æç¤ºç”¨æˆ·
  }
} else {
  stuckTimer = 0;  // é‡ç½®è®¡æ—¶å™¨
}
```

#### æ¥æºå¤±è´¥å¤„ç†

å•ä¸ªæ¥æºå¤±è´¥ä¸å½±å“æ•´ä½“ä»»åŠ¡ï¼š

```javascript
// æ£€æŸ¥æ˜¯å¦æœ‰æ¥æºå¤±è´¥
const failedSources = Object.entries(progress.sources)
  .filter(([_, info]) => info.status === 'failed')
  .map(([source, info]) => ({
    source,
    error: info.errors?.[0] || 'æœªçŸ¥é”™è¯¯'
  }));

if (failedSources.length > 0) {
  console.warn('éƒ¨åˆ†æ¥æºå¤±è´¥:', failedSources);
  // æ˜¾ç¤ºè­¦å‘Šä½†ç»§ç»­ç­‰å¾…å…¶ä»–æ¥æº
}
```

---

## PDF ä¸‹è½½é“¾æ¥æ”¯æŒ

### åŠŸèƒ½è¯´æ˜

ä» v1.3 å¼€å§‹ï¼Œæ‰€æœ‰æœç´¢ç»“æœçš„ `PaperMetadata` å¯¹è±¡éƒ½åŒ…å« `pdf_url` å­—æ®µï¼Œæä¾›è®ºæ–‡çš„ç›´æ¥ PDF ä¸‹è½½é“¾æ¥ã€‚

### å­—æ®µå®šä¹‰

```json
{
  "url": "https://arxiv.org/abs/2404.12345",        // è®ºæ–‡ä¸»é¡µ
  "pdf_url": "https://arxiv.org/pdf/2404.12345.pdf", // PDF ç›´æ¥ä¸‹è½½ï¼ˆå¯èƒ½ä¸º nullï¼‰
  "open_access": true                                 // æ˜¯å¦å¼€æ”¾è·å–
}
```

**å…³é”®åŒºåˆ«**:
- `url`: è®ºæ–‡çš„ä¸»é¡µ/è¯¦æƒ…é¡µé“¾æ¥ï¼ˆé€šå¸¸æ˜¯ HTML é¡µé¢ï¼‰
- `pdf_url`: PDF æ–‡ä»¶çš„ç›´æ¥ä¸‹è½½é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
- `open_access`: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦æœ‰å¯ç”¨çš„ PDF

### å„æ•°æ®æº PDF URL æ”¯æŒæƒ…å†µ

| æ•°æ®æº | PDF URL æ”¯æŒ | æå–æ–¹å¼ | è¦†ç›–ç‡ |
|--------|--------------|----------|--------|
| **Semantic Scholar** | âœ… å®Œå…¨æ”¯æŒ | ä» `openAccessPdf.url` æå– | ~60% (å¼€æ”¾è·å–è®ºæ–‡) |
| **OpenAlex** | âœ… å®Œå…¨æ”¯æŒ | ä» `primary_location.pdf_url` æå– | ~40% |
| **arXiv** | âœ… å®Œå…¨æ”¯æŒ | URL æ¨¡å¼è½¬æ¢ (`/abs/` â†’ `/pdf/` + `.pdf`) | 100% (æ‰€æœ‰ arXiv è®ºæ–‡) |
| **Crossref** | âŒ ä¸æ”¯æŒ | é»˜è®¤ `null` | 0% |
| **PubMed** | âŒ ä¸æ”¯æŒ | é»˜è®¤ `null` | 0% |
| **Europe PMC** | âŒ ä¸æ”¯æŒ | é»˜è®¤ `null` | 0% |

### ä½¿ç”¨ç¤ºä¾‹

#### Python æ‰¹é‡ä¸‹è½½

```python
import requests

results = search_response["results"]

for paper in results:
    if paper["pdf_url"]:
        try:
            response = requests.get(paper["pdf_url"], timeout=30)
            if response.status_code == 200:
                filename = f"{paper['year']}_{paper['title'][:50]}.pdf"
                with open(filename, 'wb') as f:
                    f.write(response.content)
                print(f"âœ“ Downloaded: {filename}")
            else:
                print(f"âœ— Failed (HTTP {response.status_code}): {paper['title']}")
        except Exception as e:
            print(f"âœ— Error downloading {paper['title']}: {e}")
    else:
        print(f"âŠ˜ No PDF available: {paper['title']}")
```

#### JavaScript å‰ç«¯æ˜¾ç¤º

```javascript
function renderPaperCard(paper) {
  const downloadButton = paper.pdf_url
    ? `<a href="${paper.pdf_url}" 
          class="btn-download" 
          target="_blank"
          rel="noopener noreferrer">
          ğŸ“¥ ä¸‹è½½ PDF
       </a>`
    : `<span class="no-pdf">æ—  PDF</span>`;

  return `
    <div class="paper-card">
      <h3>${paper.title}</h3>
      <p class="authors">${paper.authors.join(', ')}</p>
      <div class="actions">
        <a href="${paper.url}" target="_blank">ğŸ”— æŸ¥çœ‹è¯¦æƒ…</a>
        ${downloadButton}
      </div>
    </div>
  `;
}
```

### æ³¨æ„äº‹é¡¹

#### 1. PDF URL æœ‰æ•ˆæ€§

- **ä¸è¿›è¡ŒéªŒè¯**: ç³»ç»Ÿä¿¡ä»»æ•°æ®æºæä¾›çš„ URLï¼Œä¸å‘é€ HEAD è¯·æ±‚éªŒè¯
- **å¯èƒ½å¤±æ•ˆ**: PDF URL å¯èƒ½å› æƒé™ã€é‡å®šå‘ã€404 ç­‰åŸå› æ— æ³•è®¿é—®
- **å»ºè®®**: å®¢æˆ·ç«¯åº”å¤„ç†ä¸‹è½½å¤±è´¥æƒ…å†µï¼ˆè§ä¸Šæ–¹ç¤ºä¾‹ï¼‰

#### 2. `pdf_url` ä¸ `open_access` çš„å…³ç³»

```
pdf_url å­˜åœ¨ â‡’ open_access = true  (é€šå¸¸æˆç«‹)
open_access = true â‡ pdf_url å­˜åœ¨  (æŸäº›æ•°æ®æºä¸æä¾› URL)
```

**ç¤ºä¾‹**:
- Crossref å¯èƒ½æ ‡è®° `open_access=true`ï¼Œä½† `pdf_url` ä»ä¸º `null`ï¼ˆå› ä¸º Crossref ä¸æä¾› PDF é“¾æ¥ï¼‰
- arXiv æ‰€æœ‰è®ºæ–‡éƒ½æ˜¯ `open_access=true` ä¸” `pdf_url` å­˜åœ¨

#### 3. åå¤‡ç­–ç•¥

å½“ `pdf_url` ä¸º `null` æ—¶çš„æ¨èå¤„ç†ï¼š

```python
def get_pdf_link(paper):
    # ä¼˜å…ˆä½¿ç”¨ pdf_url
    if paper["pdf_url"]:
        return paper["pdf_url"]
    
    # åå¤‡ï¼šè®¿é—®ä¸»é¡µé“¾æ¥
    if paper["url"]:
        return paper["url"]
    
    # æœ€ç»ˆåå¤‡ï¼šé€šè¿‡ DOI è®¿é—®
    if paper["doi"]:
        return f"https://doi.org/{paper['doi']}"
    
    return None
```

### å®Œæ•´ç¤ºä¾‹å“åº”

```json
{
  "results": [
    {
      "title": "Attention Is All You Need",
      "authors": ["Vaswani, Ashish", "Shazeer, Noam"],
      "year": 2017,
      "journal": "NeurIPS",
      "url": "https://arxiv.org/abs/1706.03762",
      "pdf_url": "https://arxiv.org/pdf/1706.03762.pdf",
      "open_access": true,
      "citations": 95432
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": ["He, Kaiming", "Zhang, Xiangyu"],
      "year": 2016,
      "journal": "CVPR",
      "url": "https://openaccess.thecvf.com/content_cvpr_2016/...",
      "pdf_url": "https://arxiv.org/pdf/1512.03385.pdf",
      "open_access": true,
      "citations": 125789
    },
    {
      "title": "Some Paywalled Conference Paper",
      "authors": ["Smith, John", "Doe, Jane"],
      "year": 2024,
      "journal": "ACM CHI",
      "url": "https://dl.acm.org/doi/10.1145/...",
      "pdf_url": null,
      "open_access": false,
      "citations": 12
    }
  ]
}
```

---

## API ç‰ˆæœ¬å†å²

### v1.4 (2025-11-16)
- âœ¨ **å¢åŠ ç»“æœæ•°é‡ä¸Šé™**ï¼šæ”¯æŒä¸€æ¬¡è¿”å›æœ€å¤š 500 ç¯‡è®ºæ–‡ï¼ˆå¯é…ç½®ï¼‰
- âœ¨ **æ–°å¢é…ç½®**: `MAX_RESULTS_LIMIT` ç¯å¢ƒå˜é‡ï¼ˆé»˜è®¤ 500ï¼ŒèŒƒå›´ 1-1000ï¼‰
- âœ¨ **åŠ¨æ€åˆ†é¡µä¼˜åŒ–**ï¼šç§»é™¤ç¡¬ç¼–ç çš„ 100 ç¯‡é™åˆ¶ï¼ŒåŸºäº `max_results` åŠ¨æ€è®¡ç®—
- âœ¨ **æ€§èƒ½æ—¥å¿—**ï¼šè‡ªåŠ¨è®°å½• >100 ç¯‡ç»“æœçš„è¯·æ±‚æ‰§è¡Œæ—¶é—´
- ğŸ”§ **LLM æç¤ºæ›´æ–°**ï¼šæ¨è 10/20/50 ç¯‡ï¼Œæ”¯æŒæœ€å¤§ 200ï¼ˆåŒæ­¥ï¼‰æˆ– 500ï¼ˆå¼‚æ­¥ï¼‰
- âš¡ **å“åº”æ—¶é—´ä¼˜åŒ–**ï¼šæ›´é«˜æ•ˆçš„åˆ†é¡µç­–ç•¥ï¼Œå‡å°‘ä¸å¿…è¦çš„ API è°ƒç”¨
- âœ… 100% å‘åå…¼å®¹ v1.3ï¼ˆé»˜è®¤ `max_results=10` ä¸å˜ï¼‰

### v1.3 (2025-11-16)
- âœ¨ **æ–°å¢ PDF URL å­—æ®µ** (`pdf_url` å­—æ®µ)
- âœ¨ **Semantic Scholar**: ä» `openAccessPdf.url` æå– PDF é“¾æ¥ (~60% è¦†ç›–ç‡)
- âœ¨ **OpenAlex**: ä» `primary_location.pdf_url` æå– PDF é“¾æ¥ (~40% è¦†ç›–ç‡)
- âœ¨ **arXiv**: è‡ªåŠ¨ç”Ÿæˆ PDF URL (100% è¦†ç›–ç‡)
- âœ… 100% å‘åå…¼å®¹ v1.2ï¼ˆæ–°å­—æ®µä¸ºå¯é€‰ï¼‰

### v1.2 (2025-11-16)
- âœ¨ **æ–°å¢ä¸­æ–‡è¿›åº¦æè¿°** (`stage_description` å­—æ®µ)
- âœ¨ **ä¼˜åŒ–è¿›åº¦è®¡ç®—**ï¼šæ”¹ä¸ºåŸºäºå›ºå®šé˜¶æ®µçš„å¹³å‡åˆ†é…ï¼ˆ0% â†’ 25% â†’ 75% â†’ 100%ï¼‰
- âœ¨ **åŠ¨æ€æ¥æºæè¿°**ï¼šå®æ—¶æ˜¾ç¤º"æ­£åœ¨æœç´¢ {æ¥æºåç§°}"
- ğŸ“Š **æ›´ç›´è§‚çš„è¿›åº¦**ï¼šæ¯ä¸ªé˜¶æ®µæƒé‡æ¸…æ™°ï¼Œç”¨æˆ·ä½“éªŒæ›´å‹å¥½
- âœ… 100% å‘åå…¼å®¹ v1.1

### v1.1 (2025-11-16)
- âœ¨ æ–°å¢å¼‚æ­¥ä»»åŠ¡æ¥å£ (`POST /tasks/search`, `GET /tasks/{task_id}`)
- âœ¨ å®æ—¶è¿›åº¦è·Ÿè¸ªï¼ˆé˜¶æ®µ + æ¯ä¸ªæ¥æºçš„è¯¦ç»†è¿›åº¦ï¼‰
- âœ¨ è‡ªåŠ¨ TTL æ¸…ç†ï¼ˆ30 åˆ†é’Ÿï¼‰
- âœ… 100% å‘åå…¼å®¹åŒæ­¥æ¥å£

### v1.0 (2025-11-15)
- ğŸ‰ åˆå§‹ç‰ˆæœ¬
- âœ… åŒæ­¥æœç´¢æ¥å£ (`GET /search`)
- âœ… å¤šæ¥æºæ£€ç´¢ï¼ˆS2, OpenAlex, arXiv, PubMed, Europe PMC, Crossrefï¼‰
- âœ… LLM æ„å›¾è§£æ
- âœ… æ™ºèƒ½å»é‡ä¸æ’åº

---

**æ–‡æ¡£æ›´æ–°æ—¶é—´**: 2025-11-16
**API ç‰ˆæœ¬**: v1.4
**ç»´æŠ¤è€…**: PaperFinder Team

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šé€šè¿‡é¢„è®¾æŸ¥è¯¢æµ‹è¯•è®ºæ–‡æ£€ç´¢åŠŸèƒ½
è®°å½•å®Œæ•´çš„ä¸­é—´è¿‡ç¨‹ï¼šLLM è§£æç»“æœ + S2 API æŸ¥è¯¢ç»“æœ
"""

import asyncio
import json
import os
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from llm_parser import parse_user_intent
from s2_client import search_papers
from ranking import rank_papers
from schemas import SearchIntent, PaperMetadata


# ========== æµ‹è¯•ç”¨ä¾‹å®šä¹‰ ==========
TEST_QUERIES = [
    # "æ‰¾ä¸€äº›2023å¹´åˆ°2024å¹´å…³äºå¤§è¯­è¨€æ¨¡å‹çš„è®ºæ–‡ï¼Œå‘è¡¨åœ¨NeurIPSæˆ–ICLR",
    # "æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼ŒCVPRä¼šè®®ï¼Œæœ€è¿‘ä¸‰å¹´",
    # "Transformeræ¶æ„çš„æœ€æ–°ç ”ç©¶ï¼Œè¦æ±‚æœ‰PDFï¼ŒæŒ‰å¼•ç”¨æ•°æ’åº",
    # "å¤šæ¨¡æ€å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒä¸­çš„åº”ç”¨",
    # "å¼ºåŒ–å­¦ä¹ ä¸æœºå™¨äººæ§åˆ¶ï¼Œ2024å¹´ï¼ŒæŒ‰æ—¶é—´æ’åº",
    # "å›¾ç¥ç»ç½‘ç»œåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨ï¼Œéœ€è¦å¼€æºPDF",
    # "è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„few-shot learning",
    "è®¡ç®—æœºè§†è§‰ä¸­çš„å¯¹æŠ—æ ·æœ¬æ”»å‡»ä¸é˜²å¾¡",
]


class TestLogger:
    """æµ‹è¯•æ—¥å¿—è®°å½•å™¨ï¼Œä¿å­˜åˆ° JSON å’Œ Markdown æ–‡ä»¶"""
    
    def __init__(self, output_dir: str = "test_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results: List[Dict[str, Any]] = []
        
    def log_test_case(self, 
                     query: str,
                     intent: SearchIntent,
                     llm_raw_response: str,
                     papers: List[PaperMetadata],
                     stats: Dict[str, Any],
                     papers_final: List[PaperMetadata],
                     error: str = None):
        """è®°å½•å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å®Œæ•´ä¿¡æ¯"""
        
        result = {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "error": error,
        }
        
        if not error:
            result.update({
                "llm_parsing": {
                    "raw_response": llm_raw_response,
                    "parsed_intent": intent.dict() if intent else None,
                },
                "s2_api": {
                    "query_combinations": stats.get("query_combinations"),
                    "queries": stats.get("queries"),
                    "total_raw_fetched": stats.get("total_raw_fetched"),
                    "total_raw_unique": stats.get("total_raw_unique"),
                    "final_unique_count": stats.get("final_unique_count"),
                    "total_pages": stats.get("total_pages"),
                    "individual_stats": stats.get("individual_stats"),
                },
                "ranking_and_cutoff": {
                    "sort_mode": intent.sort_by if intent else None,
                    "max_results": intent.max_results if intent else None,
                    "final_count": len(papers_final),
                },
                "final_results": [
                    {
                        "title": p.title,
                        "authors": p.authors,
                        "year": p.year,
                        "venue": p.journal,
                        "citations": p.citations,
                        "influential_citations": p.influential_citations,
                        "url": p.url,
                        "has_pdf": p.open_access,
                    }
                    for p in papers_final
                ]
            })
        
        self.results.append(result)
        
    def save_results(self):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        # ä¿å­˜ä¸º JSON
        json_path = self.output_dir / f"test_results_{self.timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ“ JSON ç»“æœå·²ä¿å­˜åˆ°: {json_path}")
        
        # ä¿å­˜ä¸º Markdownï¼ˆæ›´æ˜“è¯»ï¼‰
        md_path = self.output_dir / f"test_results_{self.timestamp}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# è®ºæ–‡æ£€ç´¢æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**æµ‹è¯•ç”¨ä¾‹æ•°**: {len(self.results)}\n\n")
            f.write("---\n\n")
            
            for idx, result in enumerate(self.results, 1):
                f.write(f"## æµ‹è¯• {idx}: {result['query']}\n\n")
                
                if result.get('error'):
                    f.write(f"**âŒ é”™è¯¯**: {result['error']}\n\n")
                    continue
                
                # LLM è§£æç»“æœ
                f.write("### 1ï¸âƒ£ LLM æ„å›¾è§£æ\n\n")
                llm_data = result.get('llm_parsing', {})
                f.write("**åŸå§‹ LLM å“åº”**:\n```json\n")
                f.write(llm_data.get('raw_response', ''))
                f.write("\n```\n\n")
                
                intent = llm_data.get('parsed_intent', {})
                f.write("**è§£æåçš„æŸ¥è¯¢æ„å›¾**:\n")
                f.write(f"- æŸ¥è¯¢å…³é”®è¯ç»„: `{intent.get('any_groups')}`\n")
                f.write(f"- ç›®æ ‡ä¼šè®®/æœŸåˆŠ: `{intent.get('venues')}`\n")
                f.write(f"- ä½œè€…: `{intent.get('author')}`\n")
                f.write(f"- æ—¥æœŸèŒƒå›´: `{intent.get('date_start')}` ~ `{intent.get('date_end')}`\n")
                f.write(f"- å¿…é¡»æœ‰PDF: `{intent.get('must_have_pdf')}`\n")
                f.write(f"- è®ºæ–‡ç±»å‹: `{intent.get('publication_types')}`\n")
                f.write(f"- æœ€å°å½±å“åŠ›å¼•ç”¨: `{intent.get('min_influential_citations')}`\n")
                f.write(f"- æœ€å¤§ç»“æœæ•°: `{intent.get('max_results')}`\n")
                f.write(f"- æ’åºæ–¹å¼: `{intent.get('sort_by')}`\n\n")
                
                # S2 API æŸ¥è¯¢ç»“æœ
                f.write("### 2ï¸âƒ£ S2 API æŸ¥è¯¢\n\n")
                s2_data = result.get('s2_api', {})
                f.write(f"**æŸ¥è¯¢ç»„åˆæ•°**: `{s2_data.get('query_combinations')}`\n\n")
                
                queries = s2_data.get('queries', [])
                if queries:
                    f.write("**æ‰§è¡Œçš„æŸ¥è¯¢ç»„åˆ**:\n")
                    for i, q in enumerate(queries, 1):
                        f.write(f"{i}. `{q}`\n")
                    f.write("\n")
                
                f.write("**æŸ¥è¯¢ç»Ÿè®¡ï¼ˆæ±‡æ€»ï¼‰**:\n")
                f.write(f"- æ€»æŠ“å–æ¡æ•°: `{s2_data.get('total_raw_fetched')}`\n")
                f.write(f"- æ€»å»é‡åæ¡æ•°: `{s2_data.get('total_raw_unique')}`\n")
                f.write(f"- æœ€ç»ˆå”¯ä¸€æ¡æ•°: `{s2_data.get('final_unique_count')}`\n")
                f.write(f"- æ€»ç¿»é¡µæ•°: `{s2_data.get('total_pages')}`\n\n")
                
                # æ˜¾ç¤ºæ¯ä¸ªæŸ¥è¯¢çš„è¯¦ç»†ç»Ÿè®¡
                individual_stats = s2_data.get('individual_stats', [])
                if individual_stats:
                    f.write("**å„æŸ¥è¯¢è¯¦ç»†ç»Ÿè®¡**:\n")
                    for i, stat in enumerate(individual_stats, 1):
                        f.write(f"\næŸ¥è¯¢ {i}: `{stat.get('query')}`\n")
                        f.write(f"- æŠ“å–: {stat.get('raw_fetched')}, ")
                        f.write(f"å»é‡: {stat.get('raw_unique')}, ")
                        f.write(f"è¿‡æ»¤å: {stat.get('after_filter')}, ")
                        f.write(f"é¡µæ•°: {stat.get('pages')}\n")
                    f.write("\n")
                
                # æ’åºå’Œæˆªæ–­
                f.write("### 3ï¸âƒ£ æ’åºä¸æˆªæ–­\n\n")
                rank_data = result.get('ranking_and_cutoff', {})
                f.write(f"- æ’åºæ¨¡å¼: `{rank_data.get('sort_mode')}`\n")
                f.write(f"- è¯·æ±‚æ•°é‡: `{rank_data.get('max_results')}`\n")
                f.write(f"- æœ€ç»ˆè¿”å›: `{rank_data.get('final_count')}` ç¯‡\n\n")
                
                # æœ€ç»ˆç»“æœ
                f.write("### 4ï¸âƒ£ æœ€ç»ˆç»“æœ\n\n")
                papers = result.get('final_results', [])
                if not papers:
                    f.write("*æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡*\n\n")
                else:
                    for i, paper in enumerate(papers, 1):
                        f.write(f"#### [{i}] {paper.get('title', 'N/A')}\n\n")
                        authors = paper.get('authors', [])
                        f.write(f"- **ä½œè€…**: {', '.join(authors[:3])}")
                        if len(authors) > 3:
                            f.write(f" ç­‰ {len(authors)} äºº")
                        f.write("\n")
                        f.write(f"- **å¹´ä»½**: {paper.get('year', 'N/A')}\n")
                        f.write(f"- **ä¼šè®®/æœŸåˆŠ**: {paper.get('venue', 'N/A')}\n")
                        f.write(f"- **å¼•ç”¨æ•°**: {paper.get('citations', 0)} "
                               f"(å½±å“åŠ›å¼•ç”¨: {paper.get('influential_citations', 0)})\n")
                        f.write(f"- **å¼€æ”¾è·å–**: {'âœ“' if paper.get('has_pdf') else 'âœ—'}\n")
                        f.write(f"- **é“¾æ¥**: [{paper.get('url', 'N/A')}]({paper.get('url', '#')})\n\n")
                
                f.write("---\n\n")
        
        print(f"âœ“ Markdown ç»“æœå·²ä¿å­˜åˆ°: {md_path}")
        return json_path, md_path


async def test_single_query(query: str, logger: TestLogger):
    """æµ‹è¯•å•ä¸ªæŸ¥è¯¢"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æŸ¥è¯¢: {query}")
    print(f"{'='*60}")
    
    try:
        # 1. è°ƒç”¨ LLM è§£ææ„å›¾
        print("â³ è°ƒç”¨ LLM è§£ææ„å›¾...")
        
        # ä¸ºäº†è·å–åŸå§‹å“åº”ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹ä¸€ä¸‹è°ƒç”¨æ–¹å¼
        # ç›´æ¥ä½¿ç”¨ llm_parser ä¸­çš„ client å’Œé€»è¾‘
        from llm_parser import client, SYSTEM, _safe_json
        
        current_date = datetime.now().strftime("%Y-%m-%d")
        messages = [
            {"role": "system", "content": SYSTEM},
            {"role": "user", "content": f"å½“å‰æ—¥æœŸï¼š{current_date}\nç”¨æˆ·è¾“å…¥ï¼š{query}"},
        ]
        
        from config import OPENAI_MODEL
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.2,
        )
        llm_raw = (resp.choices[0].message.content or "").strip()
        print(f"âœ“ LLM å“åº”: {llm_raw[:100]}...")
        
        # æ‰‹åŠ¨è§£æä¸º SearchIntentï¼ˆé¿å…é‡å¤è°ƒç”¨ï¼‰
        data = _safe_json(llm_raw)
        data.setdefault("must_have_pdf", False)
        data.setdefault("max_results", 10)
        data.setdefault("sort_by", "relevance")
        intent = SearchIntent(**data)
        print(f"âœ“ è§£æå®Œæˆ: {intent.any_groups}")
        
        # 2. è°ƒç”¨ S2 API æœç´¢
        print("â³ è°ƒç”¨ S2 API æœç´¢...")
        papers, batch, stats = await search_papers(intent)
        print(f"âœ“ æœç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        # 3. æ’åºå’Œæˆªæ–­
        print("â³ æ’åºå’Œæˆªæ–­...")
        papers_sorted = rank_papers(papers, mode=intent.sort_by)
        papers_final = papers_sorted[:intent.max_results]
        print(f"âœ“ æœ€ç»ˆè¿”å›: {len(papers_final)} ç¯‡è®ºæ–‡")
        
        # 4. è®°å½•ç»“æœ
        logger.log_test_case(
            query=query,
            intent=intent,
            llm_raw_response=llm_raw,
            papers=papers,
            stats=stats,
            papers_final=papers_final,
        )
        
        # 5. æ‰“å°ç®€è¦ä¿¡æ¯
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"  - æŸ¥è¯¢ç»„åˆæ•°: {stats.get('query_combinations')}")
        print(f"  - æ€»æŠ“å–æ¡æ•°: {stats.get('total_raw_fetched')}")
        print(f"  - æ€»å»é‡å: {stats.get('total_raw_unique')}")
        print(f"  - æœ€ç»ˆå”¯ä¸€: {stats.get('final_unique_count')}")
        print(f"  - æœ€ç»ˆè¿”å›: {len(papers_final)}")
        
        if papers_final:
            print(f"\nğŸ“„ å‰3ç¯‡è®ºæ–‡:")
            for i, p in enumerate(papers_final[:3], 1):
                print(f"  {i}. {p.title[:60]}...")
                print(f"     {p.year} | {p.journal} | å¼•ç”¨: {p.citations}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        logger.log_test_case(
            query=query,
            intent=None,
            llm_raw_response="",
            papers=[],
            stats={},
            papers_final=[],
            error=str(e),
        )


async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 80)
    print(" è®ºæ–‡æ£€ç´¢ç³»ç»Ÿæµ‹è¯•")
    print("=" * 80)
    print(f"\nå…±æœ‰ {len(TEST_QUERIES)} ä¸ªæµ‹è¯•ç”¨ä¾‹\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  è­¦å‘Š: OPENAI_API_KEY æœªè®¾ç½®")
    
    if not os.getenv("S2_API_KEY"):
        print("âš ï¸  è­¦å‘Š: S2_API_KEY æœªè®¾ç½®ï¼ˆå°†ä½¿ç”¨é»˜è®¤é€Ÿç‡é™åˆ¶ï¼‰")
    
    logger = TestLogger()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    for idx, query in enumerate(TEST_QUERIES, 1):
        print(f"\n[{idx}/{len(TEST_QUERIES)}] å¼€å§‹æµ‹è¯•...")
        await test_single_query(query, logger)
        
        # é¿å…è¿‡å¿«è¯·æ±‚
        if idx < len(TEST_QUERIES):
            print("\nâ¸ï¸  ç­‰å¾… 2 ç§’...")
            await asyncio.sleep(2)
    
    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 80)
    print(" æµ‹è¯•å®Œæˆï¼Œä¿å­˜ç»“æœ...")
    print("=" * 80)
    
    json_path, md_path = logger.save_results()
    
    # ç»Ÿè®¡
    success = sum(1 for r in logger.results if not r.get('error'))
    failed = len(logger.results) - success
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"  - æ€»è®¡: {len(logger.results)} ä¸ª")
    print(f"  - æˆåŠŸ: {success} ä¸ª")
    print(f"  - å¤±è´¥: {failed} ä¸ª")
    
    print(f"\nâœ… å®Œæˆï¼è¯·æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶:")
    print(f"  - JSON: {json_path}")
    print(f"  - Markdown: {md_path}")


if __name__ == "__main__":
    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    asyncio.run(main())

